# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l887unj-e37SMOuZBzBJ0Pm3nPaInve1
"""

import requests as r
import pandas as pd
from bs4 import BeautifulSoup as bs

OPENAI_API_KEY="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"

import requests
from bs4 import BeautifulSoup
from langchain import PromptTemplate, LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI

def search_and_process(query):
    """Search the web and process the content."""
    search_url = f"https://www.google.com/search?q={query}"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(search_url, headers=headers)

    if response.status_code != 200:
        raise Exception("Failed to fetch search results")

    soup = BeautifulSoup(response.text, 'html.parser')
    # Extract meaningful content (simplified for demonstration)
    content = "\n".join([p.text for p in soup.find_all('p')][:10])
    return content

def generate_response(content, query):
    """Generate a response using LangChain."""
    memory = ConversationBufferMemory()
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.7)

    prompt_template = PromptTemplate(
        input_variables=["content", "query"],
        template="Based on the following content:\n{content}\n\nAnswer the query: {query}"
    )

    llm_chain = LLMChain(llm=llm, prompt=prompt_template, memory=memory)
    return llm_chain.run({"content": content, "query": query})

import streamlit as st
#from utils import search_and_process, generate_response

# Streamlit interface
st.set_page_config(page_title="Web Query System", layout="wide")
st.title("üåê Web Query System")

# User query input
user_query = st.text_input("Enter your query:")

if st.button("Search and Generate Response"):
    if not user_query.strip():
        st.error("Please enter a valid query.")
    else:
        with st.spinner("Processing..."):
            try:
                # Step 1: Retrieve and process web content
                content = search_and_process(user_query)

                # Step 2: Generate response using the LLM
                response = generate_response(content, user_query)

                # Display results
                st.subheader("Response:")
                st.write(response)
            except Exception as e:
                st.error(f"An error occurred: {e}")